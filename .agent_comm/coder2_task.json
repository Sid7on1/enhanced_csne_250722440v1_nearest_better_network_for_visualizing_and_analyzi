{
  "agent_id": "coder2",
  "task_id": "task_5",
  "files": [
    {
      "name": "README.md",
      "purpose": "Project documentation",
      "priority": "medium"
    },
    {
      "name": "utils.py",
      "purpose": "Utility functions",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.NE_2507.22440v1_Nearest_Better_Network_for_Visualizing_and_Analyzi",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.NE_2507.22440v1_Nearest-Better-Network-for-Visualizing-and-Analyzi with content analysis. Detected project type: computer vision (confidence score: 4 matches).",
    "key_algorithms": [
      "Ding",
      "Evolutionary",
      "Visualization",
      "Tion",
      "Gray",
      "Projection",
      "Calculation",
      "Nbn",
      "Erful",
      "Culation"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.NE_2507.22440v1_Nearest-Better-Network-for-Visualizing-and-Analyzi.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\n1\nNearest-Better Network for Visualizing and\nAnalyzing Combinatorial Optimization Problems: A\nUnified Tool\nYiya Diao, Changhe Li\u2217, Sanyou Zeng, Xinye Cai, Wenjian Luo, Shengxiang Yang, and\nCarlos A. Coello Coello, Fellow, IEEE\nAbstract \u2014The Nearest-Better Network (NBN) is a pow-\nerful method to visualize sampled data for continuous\noptimization problems while preserving multiple landscape\nfeatures. However, the calculation of NBN is very time-\nconsuming, and the extension of the method to combi-\nnatorial optimization problems is challenging but very\nimportant for analyzing the algorithm\u2019s behavior. This\npaper provides a straightforward theoretical derivation\nshowing that the NBN network essentially functions as the\nmaximum probability transition network for algorithms.\nThis paper also presents an efficient NBN computation\nmethod with logarithmic linear time complexity to address\nthe time-consuming issue. By applying this efficient NBN\nalgorithm to the OneMax problem and the Traveling\nSalesman Problem (TSP), we have made several remark-\nable discoveries for the first time: The fitness landscape\nof OneMax exhibits neutrality, ruggedness, and modality\nfeatures. The primary challenges of TSP problems are\nruggedness, modality, and deception. Two state-of-the-art\nTSP algorithms (i.e., EAX and LKH) have limitations when\naddressing challenges related to modality and deception,\nrespectively. LKH, based on local search operators, fails\nwhen there are deceptive solutions near global optima.\nEAX, which is based on a single population, can efficiently\nThis work has been accepted for publication in IEEE Transactions\non Evolutionary Computation . The final published version will be\navailable via IEEE Xplore.\nY . Diao and W. Luo are with Guangdong Provincial Key Lab-\noratory of Novel Security Intelligence Technologies, School of\nComputer Science and Technology, Harbin Institute of Technology,\nShenzhen 518055, China (e-mail: diaoyiyacug@gmail.com; luowen-\njian@hit.edu.cn).\nC. Li and X. Cai are with the School of Artificial Intelligence, Anhui\nUniversity of Science and Technology, Hefei 231131, China, and\nalso with the State Key Laboratory of Digital Intelligent Technol-\nogy for Unmanned Coal Mining, Huainan 232001, China (e-mail:\nchanghe.lw@gmail.com).\nS. Zeng is with the School of Mechanical Engineering and Electronic\nInformation, China University of Geosciences, Wuhan, China (e-mail:\nsanyouzeng@gmail.com).\nS. Yang is with the School of Computer Science and Informatics, De\nMontfort University, Leicester LE1 9BH, U.K.\nC. A. Coello Coello is with the Department of Computer Science,\nCINVESTA V-IPN, Mexico City 07360, Mexico, and also (on sabbat-\nical leave) with the School of Engineering and Sciences, Tecnol \u00b4ogico\nde Monterrey, Monterrey, N.L., Mexico.maintain diversity. However, when multiple attraction\nbasins exist, EAX retains individuals within multiple basins\nsimultaneously, reducing inter-basin interaction efficiency\nand leading to algorithm\u2019s stagnation.\nNOTICE\nThis work has been accepted for publication in IEEE\nTransactions on Evolutionary Computation . The final\npublished version will be available via IEEE Xplore.\nI. I NTRODUCTION\nCombinatorial optimization problems are a crucial\ncategory of optimization problems, prevalent in var-\nious real-world applications. The Traveling Salesman\nProblem (TSP) is a classic example of a combinatorial\noptimization problem, and many practical combinatorial\noptimization problems can be addressed using the TSP\nmodel, such as vehicle routing problems [1], DNA\nsequencing [2], and computer chip layout design [3].\nThe study of TSP is of significant importance to both\nacademia and industry.\nThere are numerous optimization algorithms for\nsolving the TSP, from which Lin-Kernighan-Helsgaun\n(LKH) [4] and Edge Assembly Crossover based GA\n(EAX) [5] are two state-of-the-art algorithms. However,\nresearch on TSP algorithms appears to have reached\na bottleneck in recent years, with few new algorithms\nshowing significant performance improvements [6]. But,\nis this really the case? As shown in experiments reported\nin [7], even for relatively simple TSP instances with\n500 cities, both EAX and LKH fail to achieve 100%\naccuracy. This indicates that certain landscape features\npose challenges that these algorithms struggle to over-\ncome. This also suggests that there is still room for\nimprovement. What we truly lack is a robust tool for\nanalyzing combinatorial problems and algorithms. Such\ntools would enable researchers to effectively identify the\ninherent difficulties in the problems, pinpoint the algo-\nrithms\u2019 weaknesses, and would allow to systematically\nimprove existing algorithms.arXiv:2507.22440v1  [cs.AI]  30 Jul 2025\n\n--- Page 2 ---\n2\nFitness Landscape Analysis (FLA) methods aim to an-\nalyze and visualize the fitness landscape through various\nsampling methods. These methods assist in landscape\nfeature analysis, algorithm performance analysis, and\nalgorithm design. In theory, a good visualization method\ncan help us observe the global and local structure of\na fitness landscape as well as the search behavior of\nalgorithms, which helps to design efficient algorithms\nfor different types of problems. However, visual FLA\nmethods for combinatorial optimization problems are\nscarce, with only three methods currently available:\nLow-Dimensional Euclidean Embedding (LDEE) [8],\nLocal Optima Network (LON) [9], and Nearest-Better\nNetwork (NBN) [10]. Previous experiments [11] have\nshown that NBN can display many landscape features\nin visualization. However, NBN lacks an efficient cal-\nculation method to handle the large number of samples\ngenerated by combinatorial algorithms and also lacks a\ntheoretical analysis to substantiate its mechanisms. Based\non this, this paper further studies NBN for combinatorial\nproblems. The main contributions of this paper are the\nfollowing:\n\u2022This paper attempts to explain the working mech-\nanism of NBN from the perspective of algorithm\nsearch behaviors with theoretical analysis. Our\npreliminary analysis shows that NBN fundamen-\ntally represents the maximum transition probability\nmodel of an algorithm. This analysis reveals that\nNBN statistically models the algorithm\u2019s behavior,\nthereby simplifying the original fitness landscape.\nMoreover, it explains why NBN can preserve most\nfeatures of the fitness landscape, which have signif-\nicant impacts on algorithm\u2019s performance.\n\u2022This paper proposes an efficient algorithm to com-\npute NBN for combinatorial optimization problems\nso that we can visualize the NBN of combinatorial\noptimization problems in logarithmic linear time\ncomplexity.\n\u2022This paper illustrates the fitness landscape structures\nof combinatorial optimization problems with differ-\nent landscape features using a tunable black-box\ndiscrete optimization benchmarking problem, the\nW-Model [12]. By adjusting parameters, the NBN\nnetwork of different W-Model instances reveals that\nthe W-Model problem exhibits ruggedness, neutral-\nity, and multimodal features.\n\u2022This paper conducts an in-depth analysis of the cur-\nrent leading TSP algorithms, EAX and LKH. Our\nexperimental results reveal that both EAX and LKH\nhave limitations when addressing challenges related\nto modality and deception, respectively. Specifi-cally, LKH, which relies on local search operators,\nstruggles with deceptive solutions near the global\noptima. On the other hand, EAX, being a single-\npopulation-based algorithm, excels at maintaining\ndiversity. However, it faces stagnation issues when\ndealing with multiple basins of attraction due to\nreduced interaction efficiency between individuals\nin different basins.\nThe remainder of this paper is organized as fol-\nlows. Section II gives an overview of previous related\nwork. Section III provides a theoretical proof of NBN.\nSection IV details the algorithm to calculate NBN for\nthe given problems. Section V presents the experimental\nanalysis of the landscape features for OneMax and TSP,\nas well as the behavior of algorithms applied to TSP.\nFinally, our conclusions and some potential paths for\nfuture research are given in Section VI.\nII. P REVIOUS RELATED WORK\nA general view of fitness landscapes was proposed\nin [13], in which the fitness landscape consists of three\nelements (X, \u03c7, f):\n\u2022A setXof potential solutions to the problem,\n\u2022a notion \u03c7of neighborhood, nearness, distance, or\naccessibility on X, and\n\u2022a fitness function f:X\u2192R. The fitness of a\nsolution indicates how good the solution is, and the\nlarger the fitness value, the better the solution.\nThere are several definitions related to the fitness\nlandscape and the algorithm\u2019s behavior, including the\nfollowing:\n\u2022Search space: The search space Xis the union of\nall possible solutions of an optimization problem.\n\u2022Neighborhood: The neighborhood relationship is\na mapping \u03c7:X\u2192N, which associates each\nsolution xwith a set of candidate solutions N(x),\n\u2022Basin of Attraction (BoA) and local optimum :\nB(x\u2217) ={x\u2208X|x\u2217=local-search (x)}, where\nthe BoA B(x\u2217)of a local optimum x\u2217is the set\nof solutions B(x\u2217)that approaches x\u2217by utilizing\na local search strategy among the decision variable\nspaceX[14].\n\u2022Search trajectory T: Search trajectory is a se-\nquence of the solutions generated by the algorithm\nin one run. In this paper, Trepresents the set of\nsolutions of a search trajectory in one run of the\nalgorithm.\nIn the past few decades, numerous FLA methods\nhave been developed, from understanding the fitness\nlandscape to guiding the search process. This section\n\n--- Page 3 ---\n3\nfocuses on the FLA methods for combinatorial optimiza-\ntion problems. These methods can be categorized into\nnon-visual and visual methods based on their ability to\nvisualize data.\nA. Non-visual methods\nNon-visualization methods are used to analyze land-\nscape features or to design benchmarks with specific\nfeatures, comparing algorithms\u2019 performance on these\nproblems to indirectly evaluate the algorithms\u2019 capability\nto address these features. The main non-visualization\nmethods include metric analysis and benchmark design.\nThese methods provide researchers with an indirect way\nto analyze algorithm\u2019s behavior, particularly when direct\nobservation of the algorithm\u2019s behavior is not available,\nhelping to gain deeper insights into algorithm\u2019s per-\nformance and challenges in real-world problem-solving\nscenarios.\n1) Metric analysis: These methods propose a series\nof metrics to describe specific features of problems or\nalgorithms. Through these metrics, the performance of\nalgorithms under different features can be evaluated,\nand the effectiveness of algorithms in solving problems\nwith similar features can be inferred. Lip used the\ncorrelation length to evaluate ruggedness: [15]. Davidor\nemployed epistasis variance to assess epistasis [16].\nReidys and Stadler utilized a neutral walk to evaluate\nneutrality [17]. Lunacek proposed the dispersion metric\nto evaluate global topology or the presence of funnels\n[18]. In theory, these metrics can be directly applied\nto combinatorial optimization problems. However, due\nto the lack of observable or quantifiable combinatorial\nbenchmarks, their performance has not been validated\nin this domain.\n2) Benchmarks design: These methods involve de-\nsigning a set of benchmarks with specific landscape\nfeatures to evaluate algorithm\u2019s performance. Bench-\nmarks may include instances with different structures\nor landscape features. By comparing how algorithms\nsolve these problems, we can reveal their adaptability\nand limitations across different features.\nIn the field of combinatorial optimization, there are\nvery few of such benchmarks available. For continuous\nproblems, we can typically verify if benchmarks ex-\nhibit the intended features through observation of the\ntwo-dimensional continuous problems [19]. However,\nevaluating whether the designed benchmarks accurately\nreflect the designed features for combinatorial problems\nis a challenging problem. W-Model [12] is the only\ncombinatorial benchmark that allows adjustments of\nruggedness, neutrality, and epistasis features.The effectiveness of the design of the benchmarks\ndepends on whether the set of designed benchmarks\nappropriately covers the range and variations of target\nfeatures. A poorly designed or incomplete benchmark set\nmay lead to misjudgments of the algorithm\u2019s behavior or\nbiased analyses.\nB. Visual methods\nTheoretically, a good visualization method can help us\nobserve both the global structure and the local structure\nof the fitness landscape, as well as the search behavior of\nan algorithm. This helps us to improve our understanding\nof the problem structure and the algorithm\u2019s working\nmechanism as well as to design efficient algorithms.\nVisual FLAs for combinatorial problems are very few:\nLON [20], LDEE [8], and NBN [10]. The neighborhood\nrelationships between solutions in the fitness landscape\nof combinatorial optimization problems are extremely\ncomplex. For instance, in a TSP problem with 500\ncities, if the neighborhood is defined based on 2-opt,\na single solution would have approximately C2\n500\u22121\u2248\n124,251neighboring solutions. One critical challenge in\nvisualizing combinatorial optimization problems is how\nto simplify these neighborhood relationships between\nsolutions.\nLON visualize the connections between local optima\nin the fitness landscape. This novel method displays the\nfitness landscape in the form of a graph where nodes\nrepresent local optima and edges indicate the transitions\nbetween optima given a specific search operator. It uses\na 2-opt local operator to search for local optima and a\n4-opt operator to escape from the current local optima,\nconstructing the local optima network in TSP. Since\nalgorithms consist of different search operators, this\nmethod can associate the algorithm\u2019s behavior with the\nstructure of the fitness landscape. LDEE shows the dy-\nnamics of the population for combinatorial problems by\nmapping combinatorial solutions into a two-dimensional\nspace using a t-distributed stochastic neighbor embed-\nding method.\nNBN can visualize data from any sampling source.\nPrevious experiments [11] have shown that NBN can\ndisplay many features of the landscape in its visualiza-\ntion. In this paper, we attempt to use NBN to visualize\nthe global structure, as well as the local structure of the\nfitness landscape, and the search behavior of algorithms\nfor two typical combinatorial optimization problems, i.e.,\nOneMax and TSP. Through NBN\u2019s visualization, we try\nto uncover some unknown difficulties of OneMax and\nTSP.\n\n--- Page 4 ---\n4\nIII. N EAREST -BETTER NETWORK\nIn this section, we provide a straightforward theoreti-\ncal analysis to explain why NBN is effective.\nTo analyze the original fitness landscape, the first\nchallenge is to find a method that can handle problems\nwith a huge number of solutions. It is an intuitive idea to\npartition the original search space into several subspaces.\nA similar method, named cell mappings techniques [21],\nis used to analyze the global behavior of nonlinear\ndynamical systems.\nLet\u2019s assume that XN={x1, ...,xN}is a big set of\nsampled solutions from the search space, where Nis a\nlarge number, and XNapproximates the whole search\nspace in this paper, X\u2192XN. Now, the fitness land-\nscape is simplified to a set consisting of a finite number\nof solutions, but the neighborhood relationships between\nevery two solutions are still complex and unknown, and\nthey still need to be simplified.\nA. A simple format of evolutionary algorithms\nThe FLAs aim to help to design efficient algorithms,\nso it is a natural way to analyze the fitness landscape\nfrom the perspective of optimization algorithms and\nthe previous section also shows that this idea helps to\nsimplify the neighborhood relationship.\nThere is a considerable number of optimization al-\ngorithms, and it is practically impossible to analyze\nthem all. Here, we consider a simple format of an\nevolutionary algorithm, a (1 + 1)-ES version, with a\nGaussian mutation operator.\nx\u2032\ni\u2190xi+N(0, r), (1)\nwhere x= [x1, x2, ..., x D],Dis the dimensionality of\nthe problem, and ris the mutation step-size.\nx=(\nx\u2032iff(x\u2032)> f(x)\nx otherwise(2)\nAlthough this type of EA is very simple, most popular\nEAs share similarities with it. For example, the Covari-\nance Matrix adaptation Evolution Strategy, (CMA-ES)\n[22] for continuous optimization problems has a similar\nformat combined with its gradient calculation method.\nAdditionally, the inner mechanism of the powerful LKH\nfor the TSP is also similar to this type of EA.\nB. Maximum Transition Network\nThen, we can calculate the transition probability be-\ntween two solutions based on Eqs. (1) and (2). Let\u2019s\nassume that the mutations of each dimension are inde-\npendent and identically distributed. Then, the mutation\nFig. 1. Mutation probability function\nprobability between two solutions aandb,pm(a\u2190b),\nis calculated as follows:\npm(a\u2190b)\n=p(a1\u2212b1)p(a2\u2212b2)...p(aD\u2212bD)\n=(2\u03c0r)\u2212D\n2exp(\u2212\u2225a,b\u2225\n2r),(3)\nFig. 1 shows the mutation probability function pm(a\u2190\nb)associated with \u2225a,b\u2225andr. Generally speaking, the\nmutation step-size ris a pre-defined parameter.\nThe selection probability ps(a\u2190b)is calculated by\nps(a\u2190b) =(\n1,iff(a)> f(b)\n0, otherwise, (4)\nFinally, the transition probability between the two solu-\ntions p(a\u2190b)is calculated by\np(a\u2190b)\n=pm(a\u2190b)ps(a\u2190b)\n=\uf8f1\n\uf8f2\n\uf8f3(2\u03c0r)\u2212D\n2exp(\u2212\u2225a,b\u22252\n2r),iff(a)> f(b)\n0, otherwise(5)\nNow that the relationship between every two solutions\nis known to us, theoretically, we can analyze the fitness\nlandscape according to these equations. However, if\nwe consider all the relationships, the fitness landscape\nanalysis will be too complex to be applied to any\nhigh-dimensional or combinatorial problem. So, we try\nto simplify the relationship by maintaining only the\nmaximum transition relationship for each solution. In the\nnetwork, \u03b2(x)is the solution with maximum transition\nfrom solution x, which is defined as:\n\u03b2(x) = arg max\ny\u2208XNp(y\u2190x)\n= arg min\ny\u2208{y|y\u2208XN,f(y)>f(x)}\u2225y,x\u2225(6)\n\u03b2(x)is also known as the nearest better solution in [23].\nNote that for the global optimum o, there is no better\nsolution.\n\n--- Page 5 ---\n5\nFig. 2. The construction of maximum transition network with a step-\nsizer\nWith the simplification of the original fitness land-\nscape and the nearest better relationship, the maximum\ntransition network can be defined as a directed graph\nG= (V,E), where the set of vertices is the set\nof representative solutions, V=XN, and the set of\nedges is the nearest better relationship for every solution,\nE={(x, \u03b2(x))|x\u2208XN}.\nNBN fundamentally represents the maximum transi-\ntion network. This straightforward proof explains why\nNBN is effective: it simplifies the structure of the fitness\nlandscape while still preserving its essential features. It is\nworth noting that NBN is not equivalent to the maximum\ntransition network with the step-size r. As shown in\nFig. 2, when \u2225x, \u03b2(x)\u2225> r, the connection between two\nsolutions is severed.\nIV. C ALCULATION OF THE NEAREST -BETTER\nNETWORK\nThe first version of NBN was proposed in [10],\nwhere the NBN is generated by traversal algorithms\n(CNBSI), in which the time complexity is O( N2D)\n(Nis the number of sampled solutions and Dis the\ndimensionality of the problem). Here, we provide a\nmore efficient algorithm to calculate the network for the\nassignment problem, which represents a typical type of\ncombinatorial problem [24], e.g., TSP and OneMax.\nA. Distance metric\nFrom the definition of NBN, the relationship between\ntwo solutions is defined based on the distance between\ntwo solutions, and the distance metric is different for\ndifferent problems.\nIn One-Max problems with Ddigits, the Hamming\ndistance is used as the distance metric. In the symmetric\nTSP with Dcities, the Dice coefficient [25] is used as\nthe distance metric. In the symmetric TSP, the distance\nbetween two solutions aandbdefined by the Dice\ncoefficient is:\n\u2225a,b\u2225= 1\u22122|M(a)TM(b)|\n|M(a)|+|M(b)|(7)where for a solution a= [a1, a2, ..., a D],M(a) =\b\n(ai, a(i+1)% D),(a(i+1)% D, ai)|i= 1,2, ..., D\t\nis the\nset of all edges that connect two cities for the solution.\nThe neighborhood is defined according to the corre-\nsponding distance definition:\nN(x) ={y| \u2225x,y\u2225< r,\u2200y\u2208XN} (8)\nwhere ris a predefined value.\nB. Problem representation\nWe now consider the assignment problem [26], which\nwidely exists in real-world applications. In the model,\nthe search space is defined by a finite set of variables,\nx\u2208V1\u00d7V2\u00d7, ...,\u00d7VD, and each variable of the\nsolution xihas an associated domain Viof values that\ncan be assigned to it. A solution x= [x1, x2, ..., x D]is\nan assignment of a value v\u2208Vito variable xiand it is\ndenoted by xi=v. In a TSP with Dcities, a variable\nxiin a solution is a move from city ito the city xi.\nThis can be formalized by associating one variable to\neach city and each variable xihas then D\u22121associated\nvalues, xi\u2208Vi={a|a= 1,2, ..., D, a \u0338=i}. In a\nOneMax problem with Ddigits, a variable xiindicates\nthe digit at the ithdimension, xi\u2208Vi={0,1}.\nC. The proposed algorithm\nInspired by the random projection technique [27],\nwhich serves as an efficient dimensionality reduction\ntechnique for combinatorial problems, we propose a\nsimilar method to speed up the calculation.\nIn the proposed method, we divide the solution set\ninto several smaller solution sets by one of the domains\nassociated with one dimension, until the solution set is\nsmall enough to calculate their nearest better solutions,\nnormally with Nm= 20 solutions. For one divided\nsolution set, if it is divided into multiple subsets, it\ngathers the best solution from each subset to calculate\nits NBN by CNBSI; otherwise, the NBN is calculated\ndirectly by Algorithm 1.\nAs stated in Line 8 of Algorithm 1, a solution set S\nis randomly divided into multiple subsets. and CNBSD\nis used to compute the best solution and nearest better\nrelationships ( \u03b2) for each subset. For the nearest better\nrelationships of the current solution set S, the nearest\nbetter relationships \u03b2for the solutions within each subset\nhave already been calculated, as shown in Line 10. For\neach subset\u2019s best solution, the nearest better solution\n(excluding the best solution of the current solution set)\nmust belong to the current solution set, which must be\nthe best solution of the other subsets. Therefore, we can\nuse CNBSD to compute the nearest better relationships\n\n--- Page 6 ---\n6\nAlgorithm 1: Calculation of nearest better solutions\nby division (CNBSD)\nInput: A set of sampled solutions S,\nthe set of all the unselected variable domain set DV,\nand the minimum number of the size of the calculated\nsetNm\nOutput: The nearest better relationship \u03b2, and the best\nsolution bofS.\n1:if|S| \u2264Nmthen\n2: \u03b2=CNBSI (S)\n3:b= arg max a\u2208Sf(a)\n4:else\n5: Record all the best solutions for each subset P=\u2205\n6: Randomly select a domain set VkfromDV\n7:DV=DV\u2212 {Vk}\n8: DivideSby the kthdomain set Vkinto different\nsubset,S=S1\u222a...\u222aSt\n9: foreach subset Si\nxdo\n10: (\u03b2i,bi) =CNBSD (Si, DV, Nm)\n11: \u03b2= min( \u03b2, \u03b2 i)\n12: P=P\u222a {bi}\n13: end for\n14: \u03b2b=CNBSI (P)\n15: \u03b2= min( \u03b2, \u03b2 b)\n16: b= arg max a\u2208Pf(a)\n17:end if\nfor the set of best solutions Pfrom these subsets, as\nshown in Line 14.\nFinally, by merging the results of the nearest better\nrelationships (i.e., select the closest better solution for\neach solution), we obtain the nearest better relationship\n\u03b2for the current solution set S. With more random par-\ntitionings, the accuracy of the nearest better relationship\ncalculation is more accurate. This is done in Algorithm 2.\nActually, dividing the solution set into several subsets\nis a projection for the solutions from the original search\nspace to a new specific dimension, and there exists an\nerror in the calculation of the nearest neighbors with\nonly one projection. According to Johnson-Lindenstrauss\nlemma [28], we can calculate NBN with Nsolutions\nwith the minimum number of projections, L, and the\ndesired error limit \u03f5, such that the solutions can be\nprojected with a high probability based on a random\nprojection:\nL >ln(N)\n\u03f52(9)\nThus, we can guarantee that the error of Algorithm. 2 is\nsmaller than \u03f5withL >ln(N)\n\u03f52times of projections.\nGenerally, for a set of Nsampled solutions, in a\nsingle random projection, we only need to partition a\nsolution set Ntimes to completely distinguish any two\nsolutions. For each pair of solutions, we compute theAlgorithm 2: Calculation of the nearest better solu-\ntions by random projection (CNBSRP)\nInput: A set of sampled solutions S,\nthe number of loops for calculation L,\nand the minimum number of the size of the\ncalculated set Nm\nOutput: The nearest better relationship beta\n1:fork\u21901toLdo\n2: Initialize DV:DV={V1,V2, ...,VD}\n3:(\u03b2i,bi) =CNBSD (S, DV, Nm)\n4: Update \u03b2:\u03b2i= min( \u03b2, \u03b2i)\n5:end for\ndistance between them to calculate NBN. Therefore, the\ntime complexity of this algorithm isNln(N)D\n\u03f52, where D\nis the dimensionality of the problem, e.g., the number of\ncities in a TSP instance.\nFig. 3. Running time of the two algorithms, where both algorithms\nare implemented using multithreading in a system equipped with an\nIntel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz, featuring 88 cores.\nAs shown in Fig. 3, compared to the previous NBN\nalgorithm [10] with a time complexity of O(N2D),\nthis algorithm has significantly reduced the time com-\nplexity. Moreover, this algorithm is parallelizable. In\nAlgorithm. 2, we can parallelize the processing of each\nrandom projection, which is beneficial for optimizing the\nuse of computing resources and for accelerating the NBN\ncalculation.\nD. Local sampling\nThe solution space of a combinatorial optimization\nproblem typically consists of a large number of solutions.\nFor example, in a TSP with Dcities, the number of\nsolutions is (D\u22121)!/2. If we perform global sampling\nwith only Nsampled solutions, the distribution of the\nsampled solutions is relatively sparse compared to the\nentire solution space, and we can only observe the global\nstructure of the problem. The local structure of the\n\n--- Page 7 ---\n7\nproblem is almost impossible to observe. To observe the\nproblem from different scales, we can perform a local\nsampling in a local area around a center solution xc\nwithin a distance of K. The local area around a center\nsolution xcwithin a distance of K,S(xc, K), is defined\nby:\nS(xc, K) ={a\u2208X| \u2225a,xc\u2225 \u2264K} (10)\nThe proposed algorithm can directly calculate the\nNBN structure for local region sampled solutions. How-\never, in our experiments, we found out that our algo-\nrithms perform well when the sampled solutions are\nevenly distributed. But when the distribution of sampled\nsolutions is concentrated, some subsets may contain\nmany solutions and it is time-consuming to divide such\nsubsets. In a local sampled solutions set, many solutions\nshare the same values with the center solution xc, and\nthus we remove the subset that contains xcand this is\ndone after Line 8 of Algorithm 1.\nV. NBN ASSISTED ANALYSIS OF COMBINATORIAL\nPROBLEMS AND ALGORITHMS\nA. Analysis metric\nIn our previous work [11], we introduced several\nmetrics for evaluating landscape features based on NBN,\nincluding modality, BoA, ruggedness, and neutrality.\nThese metrics were originally proposed under the as-\nsumption of a uniformly distributed dataset. However,\nlocal or algorithmic sampling data is non-uniform, ren-\ndering the previously proposed metrics inapplicable to\nthe biased data-based NBN scenarios. In this subsection,\nwe introduce some new metrics that do not rely on\nthe assumption of uniform data distribution, thereby\nassisting in analyzing problems and algorithms.\n\u2022Evolutionary path ( \u02dcP)\nNBN, G= (V,E), is essentially a tree structure,\nwhere each solution is attracted by only one nearest\nbetter solution except for the global optima. For\neach solution x\u2208V, there exists a path connecting\nxto the global optima o. Considering that NBN\nis the maximum transition probability network, it\ncan be proven that this path represents the evolution\npath with the highest probability for the solution x\nto converge to the global optimum. The definition\nof this evolutionary path is defined as follows:\n\u02dcP(x,o) = [p1,p2, . . . ,pk]where p1=x,pk=o,\nandkrepresents the number of nodes along the\npath.\n\u2022Distance of an evolutionary path ( d(\u02dcP))\nFor any given evolutionary path \u02dcP, its complexity\ncan be gauged by the maximum distance betweenits nodes. An increased maximum distance implies\na reduced likelihood of transitioning to the subse-\nquent node, thereby indicating a higher degree of\ndifficulty of the evolutionary path. This measure is\nquantified as the distance of the evolutionary path in\nthis paper, defined as d(\u02dcP) = maxk\u22121\ni=1\u2225pi,pi+1\u2225.\n\u2022Distance of a solution set to the optima ( d(T,o))\nEvolutionary algorithms focus on solutions with\neither higher fitness values or greater evolutionary\npotential. As long as there is one solution in the\nsolution set, T, that possesses a shorter evolutionary\npath, the algorithm based on this set is more likely\nto converge to the global optimum.\nAccordingly, this paper defines the distance of the\nshortest evolutionary path of a solution set as the\ndistance between a solution set and the global\noptimum, denoted as:\nd(T,o) = min\nt\u2208Td(\u02dcP(t)), (11)\nFurthermore, the shortest evolutionary path from the\nsolution set to the global optimum is given by:\n\u02dcP(T,o) =\u02dcP(t),t= arg min\nt\u2208Td(\u02dcP(t)) (12)\n\u2022Identification of optima in biased data-based NBN\nIn our previous work [11], optimal solutions are\nidentified based solely on the magnitude of the\nNearest-Better Distance (NBD) of the solutions.\nHowever, this approach is not suitable for the NBN\ngenerated from biased data. The distribution of data\nevolved by the algorithm is non-uniform. Early in\nthe evolutionary process, the search radius of the\nalgorithm is relatively large, leading to larger NBD\nvalues in some poorer regions. Consequently, some\nsolutions may be mistakenly judged as local optima\ndue to their large NBD.\nOptimal solutions are inherently those with better\nfitness values. In the biased data-based NBN, fitness\nand NBD are integrated to identify optima, as\nshown in:\nf(x)\u2265\u03b8\u2227dNBD(x)\u2265\u03d1 (13)\nB. Feature analysis of the OneMax problem\nThe W-Model [12] is the only combinatorial bench-\nmark that can adjust the degrees of ruggedness, neutral-\nity, and epistasis. By setting specific parameters, we can\nmodify the degree of the three features of a W-Model\nfunction. The experiments in [12] showed that these\nparameters indeed affect the problem-solving difficulty.\nHowever, there is currently no research proving whether\nthese parameters influence difficulty by altering the\n\n--- Page 8 ---\n8\norigin Ruggedness \u03b3= 4356 Neutrality \u00b5= 24 Epistasis \u03c5= 14\nK= 120 K= 120 K= 120 K= 120\nK= 30 K= 30 K= 30 K= 30\nK= 7 K= 7 K= 7 K= 7\nFig. 4. Visualization of NBN on 120 bits Tunable W-Model of different features from different scales\ncorresponding landscape features or by changing other\nfeatures. This subsection constructs W-Model functions\nwith different parameters and analyzes the global and\nlocal structures of the problem to analyze the correlation\nbetween these parameters and these features.\nIn this experiment, we construct W-Model functions\nwith 120 digits with different parameters, where \u03b3,\u00b5,\nand\u03c5represent the degree of ruggedness, neutrality,\nand epistasis, respectively. Fig. 4 shows the impact of\ndifferent parameters on the global and local structures of\nthe fitness landscape of W-Model functions. TABLE I\nshows that the number of optima is quite different\non different W-Model functions based on Eq.(13) with\n\u03b8= 9 and\u03d1= 20 . In the experiments, the number of\nsamples is set to N= 106, and Kis the radius of the\nlocal area for sampling as Eq. (10).\nWe analyze these W-Model functions based on the\nfollowing aspects:\n\u2022Ruggedness\nAs depicted in Fig. 4, the NBN of the local structure\nof all W-Model functions, except for the neutral\nfunctions, exhibit a rugged landscape, with straight\nhanging branches at the bottom [11]. This aligns\nwith the inherent understanding that the fitnesslandscapes of combinatorial problems are rugged\nand also explains why local search operators are\ncrucial for these problems.\nHowever, no remarkable differences are observed\nin the NBN visualization between the W-Model\nfunctions with \u03b3= 0 and those with \u03b3= 4356 .\nFurthermore, TABLE I indicates that the parameter\ninfluences the number of optima, but it does not\nshow a significant correlation between \u03b3and the\nnumber of optima.\n\u2022Neutrality\nFrom NBN of the neutral W-Model functions with\n\u00b5= 24 in Fig. 4, We can see several flat regions, in-\ndicating the function\u2019s neutrality feature. Moreover,\nTABLE I shows that with a larger \u00b5, the function\nhas a larger number of optima with the same fitness.\nThis shows that the parameter \u00b5indeed affects the\ndegree of neutrality of the functions.\n\u2022Epistasis\nFrom Fig. 4, we can see that the parameter \u03c5\nessentially affects the number of optima and the size\nof BoAs, especially in the local structure of K= 7.\nFurthermore, TABLE I corroborates that there is a\npositive correlation between the \u03c5and the number\n\n--- Page 9 ---\n9\nrue500-1\nK= 500 K= 200 K= 50 K= 12\ntop view top view top view side view\nside view side view side view side view\nFig. 5. Visualization of NBN of rue500-1 from different scales with N= 106solutions generated by local random sampling, Kis the\nradius of the local region.\nTABLE I\nIDENTIFICATION OF THE NUMBER OF OPTIMA FOR W-M ODEL OF\nDIFFERENT FEATURES FROM DIFFERENT SCALES\nRuggedness\n\u03b3\\K 7 15 30 60 120\n0 73 12 10 36 20\n1452 65 12 16 21 19\n2904 59 15 19 29 27\n4356 60 9 18 29 18\n5808 86 8 15 40 25\n7260 69 11 12 23 24\nNeutrality\n\u00b5\\K 7 15 30 60 120\n0 73 12 10 36 20\n12 451061 258715 91916 22476 7440\n24 453886 323253 194666 105314 65503\n36 942718 809168 565516 336946 180960\n48 994114 957815 831526 612520 309765\n60 988072 932624 774689 542237 303673\nEpistasis\n\u03c5\\K 7 15 30 60 120\n0 73 12 10 36 20\n2 34 14 11 27 57\n6 98 34 52 114 165\n10 401 89 110 154 179\n14 508 95 125 173 178\n18 397 66 107 150 201\n22 922 148 134 168 165\nof optima. This indicates that the parameter indeed\naffects the degree of epistasis of the functions.C. Feature analysis of the traveling salesman problem\nThis subsection aims to analyze the global and local\nstructures of the fitness landscapes of three typical TSP\ninstances and the search data of LKH and EAX. By\ncombining this with the landscape features, this paper\naims to identify the challenges for the algorithms in\nsolving these problems. With the analysis, this paper\naims to provide insights for the design and optimization\nof algorithms.\n1) TSP instances selection: TSPlib [29] is a widely\nused benchmark dataset and the portgen generator [30]\ngenerates TSP instances (referred to as a run instance)\nby randomly placing points on a two-dimensional plane.\nWe select three typical TSP instances: u574, rue500-\n1, and rue500-2. u574 is a commonly used TSP instance\nfrom TSPlib, containing 574 nodes. This instance was\nalso used for observation and analysis in the paper of\nLON [9]. Researchers can compare the results of this\npaper with those of LON [9] to deepen the understanding\nof the fitness landscape of the TSP instances. Both\nrue500-1 and rue500-2 have 500 nodes generated by\nthe portgen generator. TABLE II shows the success rates\nof EAX and LKH, i.e., the number of runs that found\nthe global optimum versus the total number of runs,\nwhere both algorithms use the recommended parameters.\nAs shown in TABLE II, the performance of LKH and\nEAX on the two TSP instances is the opposite. They\nboth have 500 nodes, why do EAX and LKH behave\nso differently? The analysis of this subsection tries to\nanswer this question.\n\n--- Page 10 ---\n10\nTABLE II\nSUCCESS RATE OF EAX AND LKH ON DIFFERENT TSP\nINSTANCES\nu574 rue500-1 rue500-2\nEAX 30/30 2/30 30/30\nLKH 30/30 30/30 4/30\nTo answer the question above, we first take a look\nat the working mechanism of the two algorithms. LKH\nmainly consists of three techniques: local search op-\nerators, restart, crossover, and a GA framework. LKH\nwill restart several times and in each restart, it generates\na random solution, optimizes this solution using local\nsearch operators, and crosses over this solution with the\niteration-best solution or a solution in the GA population\nto find a better solution. The key to EAX lies in its GA\nframework and an efficient crossover operator. It is a\nsingle-population algorithm that maintains a certain level\nof diversity during the evolutionary process.\n2) Global and local structure of TSP: By comparing\nthe results of the three TSP instances in Fig. 5, Fig. 6,\nand Fig. 7, we can find several features:\n\u2022Ruggedness\nThe TSP instance exhibits ruggedness from global\nto local regions with numerous straight hanging\npoints. While in Fig. 5, Fig. 6, and Fig. 7, we\ncan see that whether it\u2019s NBN with LON data\nor NBN with total data, the fitness landscape has\nbeen considerably smoothed out. This indicates that\nlocal search operators can smoothen the structure of\nthe TSP fitness landscape, which also validates the\nimportance of local search operators for TSP.\n\u2022Modality\nFrom the results in Fig. 5, we can see that rue500-1\nexhibits a single BoA globally, with multiple BoAs\nemerging in the local structure when Kis smaller\nthan 50. In the local structure of K= 12 , there are\ntwo connected BoAs.\n3) Comparison between LON and NBN: For the same\ndataset, the structures of LON and NBN exhibit similar-\nities as shown in Fig 6 and Fig 7. Due to the high time\ncomplexity of the force-directed algorithm used in LON\n(N2, where Nis the number of solutions), it can only\nvisualize the best 0.01% of solutions. The similarities\nbetween LON and NBN suggest that although NBN\nonly preserves the nearest-better relationship within the\nnetwork, it still retains the features of BoAs.\nLON relies on local search operators to explore rela-\ntionships between solutions, which smoothens the fitness\nlandscape, whereas NBN can visualize solutions from\nany source and retains important features such as rugged-ness as illustrated in Fig. 5. The experiments below also\nshow that NBN provides more information for a more\nprofound analysis of the problem features and algorithm\nbehaviors.\n4) Algorithm behavior analysis: To compare the ca-\npabilities of LON, LDEE, and NBN in analyzing algo-\nrithm\u2019s behavior, we evaluate TSP instances using the\ndata generated by each tool individually.\n\u2022Analysis based on LON\nLON can display the connections between local\noptima and further illustrate the BoAs. As illustrated\nin Fig. 6 and Fig. 7, all three TSP instances have two\nBoAs, and in rue500-1, most regions of the BoAs\nare connected. It seems that rue500-1 is easier than\nthe other two instances. But is that correct? From\nthe results in TABLE II, LKH has a low success rate\non rue500-2. LON does not provide a clear answer\nas to whether the algorithm consistently gets stuck\nin the red local optima, as shown in Fig. 7.\nOn the other hand, both u574 and rue500-2 have\ntwo separate BoAs, yet EAX consistently finds the\nglobal optimum on the two instances. However,\nin rue500-1, which appears simpler with two con-\nnected BoAs, the algorithm\u2019s success rate is quite\nlow. Based on LON visualization, it is difficult\nto identify the specific factors causing the poor\nperformance of EAX in this instance.\n\u2022Analysis based on LDEE\nLDEE maps all solutions onto a two-dimensional\nplane by minimizing the total distance between each\npair of solutions. From the visualization, we can see\nthat much information is lost, making it difficult to\ninfer details about the landscape features. We can\nonly observe the relative positions of the solutions\nand their fitness values.\nLDEE focuses on minimizing the overall relative\ndistances between all pairs of solutions, which may\ncause the loss of some critical information. The\ninformation about the distance between optima is\nof great importance to the researcher. As shown in\nFig. 6, in NBN of u574, four optima are very close\nto each other. In LON of u574, the four optima are\nalso mutually reachable. In the LDEE visualization\nwith total data, the positions of the four optima\nare close. However, in the LDEE visualization with\nLON and EAX data, we can see two sets of distant\noptima (white rectangles), which can be misleading,\nsince one may believe that there are two groups of\noptimal solutions far apart in u574. The difference\nbetween LDEE with total data and with LON, EAX\ndata also indicates that LDEE\u2019s mapping involves a\nsignificant degree of randomness.\n\n--- Page 11 ---\n11\nu574\nLON NBN, LON data NBN, total data LDEE, EAX\u2019s success, T0\ntop view top view top view total data\nside view side view side view LON and EAX data\nrue500-1\nLON NBN, LON data NBN, total data LDEE, total data\ntop view top view top view LKH \u2019s success, T0\nside view side view side view EAX \u2019s failure, T9\nFig. 6. Comparison of NBN, LON, and LDEE on u54 and rue500-1: Recommended parameters [9] are used in LON and figures of LDEE\nare generated by the tool provided by its authors [8]. There are three sources of data: data generated by LON, EAX, and LKH, that is LON\ndata, EAX data, and LKH data. Total data is the union of all these data. Each point in LON represents a local optimum, and connections\nbetween points indicate transitions between the solutions by the 4-opt operator. The height of each point indicates its fitness value (lower\nvalues indicate better fitness). The color of points (from blue to red) represents the basin to which they belong, while gray points belong to\nmultiple basins. Black rectangles are the global optima. This color-coding scheme is the same for NBN. LDEE is a two-dimensional grid\nimage in which each grid represents a solution. The color gradient from blue to red indicates the fitness value of solutions (red is the best).\nWhite rectangles denote the locations of the optima, and black circles are the solutions of an algorithm\u2019s trajectory T.Tiindicates the ith\ntrajectory of an algorithm.\nFurthermore, it is hard to draw any effective con-\nclusions about algorithm behavior from the LDEE\nvisualization. In the case of EAX\u2019s failure on\nrue500-1, we can see that EAX finds some solutions\nvery close to the global optimum. But despite this\nproximity, why does EAX fail to find the global\noptimum? LDEE does not provide answers to thisquestion.\nSimilarly, in the case of LKH\u2019s failure in rue500-2,\nLDEE shows that the solutions generated by LKH\nare close to the global optimum as shown in Fig. 7.\nIt seems that LKH with the nearest solution can\nconverge to the global optimum using any local\nsearch operator. But is this the case? The following\n\n--- Page 12 ---\n12\nrue500-2\nLON NBN, LON data NBN, total data LDEE, total data\ntop view top view top view LKH \u2019s failure, T0\nside view side view side view EAX \u2019s success, T0\nFig. 7. Comparison of NBN, LON, and LDEE on rue500-2\nNBN-based analysis shows that in this trajectory\nT0, LKH gets stuck in a deceptive funnel.\n\u2022Analysis based on NBN\nTo better analyze the behaviors of the two algo-\nrithms, different trajectories are shown in Fig. 8.\nThe statistical information of d(\u02dcP(T,o))is shown\nin TABLE III, where \u201cFailed TSP instance\u201d indi-\ncates the instances where they have a low success\nrate. \u201cmin\u201d, \u201cmax\u201d, \u201cmean\u201d, and \u201cSD\u201d represent\nthe minimum value, maximum value, mean value,\nand standard deviation of d(\u02dcP(T,o)), respectively.\n\u201c Fails (Deceptive/Total)\u201d indicates the number of\nfailures when the algorithm gets stuck in deceptive\nsolutions versus the total number of failures.\n\u2013EAX\u2019s behaviors\nFrom TABLE II, we see that EAX fails only on\nrue500-1. In Fig. 8, we observe that solutions\nexist within the BoAs of the global optima\nin both success and failure cases. Even in\ntrajectory T15when the distance to the optima\nis relatively large, d(\u02dcP(T15,o)) = 17 , there\nstill exist several solutions in BoA of the global\noptima. EAX does not suffer from a lack of\ndiversity in detecting the BoA of the optimum.\nOn the contrary, it successfully locates the BoA\nof the global optima.\nMoreover, TABLE III shows that d(\u02dcP(T,o))\nvaries significantly across different trajecto-\nries, suggesting that the algorithm converges to\ndifferent locations in these trajectories, which\nalso validates the ability of EAX to maintaindiversity.\nAlthough EAX can detect the BoA of global\noptima in all these trajectories, it still has a\nlow success rate on rue500-1 as shown in\nTABLE II. Then, we further analyze EAX\u2019s\nbehavior on rue500-1. In the 9thtrajectory\nT9, the distance between T9and the global\noptima ois only 3. It seems that the local\nsearch operators applied to the nearest solution\ncould easily find the global optimum. However,\nEAX relies on edge assembly crossover to\nimprove the solutions. When multiple BoAs\nexist, EAX retains individuals within multiple\nbasins simultaneously, reducing inter-basin in-\nteraction efficiency and leading to algorithm\u2019s\nstagnation. The result of the optima screened\naccording to Eq. (13) with \u03b8= 0.99and\n\u03d1= 30 also supports this conclusion, where\nrue500-1 has 5 optima, while the other two\ninstances have only 2 optima. All the optima\nare marked as circles in Fig. 8.\n\u2013LKH\u2019s behaviors\nAs shown in TABLE II, LKH struggles with\nrue500-2. Compared to the EAX\u2019s behaviors,\nwe know that modality is not the challenge that\nLKH encounters.\nIn Fig. 8, we can see that in rue500-2 LKH\ntends to converge to the local optima and\nthe blue diamond-shaped solution. Even in the\nLKH\u2019s successful trajectory, T3, there are many\nsolutions around the blue diamond-shaped so-\n\n--- Page 13 ---\n13\nu574\nLKH\u2019s success, T0 EAX\u2019s success, T0\ntop view side view top view side view\nrue500-1\nLKH\u2019s success, T0 EAX\u2019s success, T12\ntop view side view top view side view\nEAX\u2019s failure, T9,d(T9,o) = 3 EAX\u2019s failure, T15,d(T15,o) = 17\ntop view side view top view side view\nrue500-2\nLKH\u2019s success, T3 EAX\u2019s success, T12\ntop view side view top view side view\nLKH\u2019s failure, T0,d(T0,o) = 15 LKH\u2019s failure, T5,d(T5,o) = 7\ntop view side view top view side view\nFig. 8. NBN visualization with total data, where the gray network is NBN, colored stars are the solutions of an algorithm\u2019s trajectory T.\nTiindicates the ithtrajectory of an algorithm. Black circles indicate the local optima and red circles are the global optima o. Diamond\nmarkers indicate deceptive solutions. Black rectangles are the solutions along the shortest evolutionary path from the trajectory Tto the\nglobal optima \u02dcP(T,o).d(T,o)is the distance of the evolutionary path. For EAX\u2019s trajectories, the color represents the generated iteration\nof the solutions. For LKH\u2019s trajectories, the color represents the number of runs that the solutions are generated.\nlution. Only one set of solutions (orange stars) generated in the same run are situated around\n\n--- Page 14 ---\n14\nTABLE III\nSTATISTICAL INFORMATION OF d(\u02dcP(T,o))OVER THE 30INDEPENDENT TRAJECTORIES FOR THE TWO ALGORITHMS ON THEIR FAILED\nTSP INSTANCES .\nAlgorithm Failed TSP instance min max Avg \u00b1 SD Fails (Deceptive/Total)\nEAX rue500-1 3 17 9.25\u00b13.94267 -/28\nLKH rue500-2 7 15 14.6923\u00b11.53846 25/26\nTABLE IV\nSTATISTIC DATA OF THE FUNNELS AROUND OPTIMA ,K= 16\nId Decep. \u2225n,o\u2225 NBDN= 10,000 N= 100 ,000 N= 1,000,000\n\u2206f d(S(n, K),o) \u2206f d(S(n, K),o) \u2206f d(S(n, K),o)\nu5741 15 11 6.86E-03 37 6.63E-03 34 7.26E-03 34\n2 17 12 9.09E-03 40 5.49E-03 41 5.19E-03 45\n3 17 13 -1.47E-02 38 -1.04E-02 41 -1.08E-02 32\nrue500-11 12 12 2.57E-03 32 6.85E-04 37 1.88E-04 34\n2 17 10 2.56E-03 41 7.59E-04 38 1.28E-03 37\nrue500-21 17 14 1.73E-02 44 1.69E-02 39 1.63E-02 36\n2 11 11 8.14E-04 36 2.98E-04 34 7.06E-04 33\n3 13 13 -1.83E-03 41 5.81E-04 36 7.82E-04 32\n4\u221a15 15 -1.43E-03 38 -4.43E-04 35 -6.69E-04 30\n5 16 12 3.10E-04 40 4.18E-04 36 3.71E-04 31\nthe global optima. Besides, EAX also has many\nsolutions around the blue diamond-shaped so-\nlution. It seems that EAX treats it like a local\noptimum.\nThe data in TABLE III also corroborates this\nphenomenon. Among the 26 failures, LKH\nis stuck in the blue diamond-shaped solution\n25 times. Then, is the solution deceptive? To\nverify this hypothesis, we need to answer two\nquestions: (1) Why is that the other two in-\nstances do not have deceptive solutions? (2)\nWhy is the blue diamond-shaped solution the\nonly deceptive solution in rue500-2?\nWe know that a deceptive solution is a solution\nthat is close to the global optimum with better\nBoAs, so algorithms are attracted by the decep-\ntive solution and thus ignore the global optima.\nBased on this, we filter out all the potential de-\nceptive solutions in all three instances as shown\nin TABLE IV. The largest local search operator\nused by LKH is the 5-opt, which indicates that\nLKH can find the best solutions in a local area\nwith a radius K\u226410. For a solution with NBD\nsmaller than 10, LKH can find its nearest better\nsolution using the 5-opt local search operator.\nThus, any possible deceptive solution should\nhave an NBD larger than 10 so that LKH can\nconverge to it instead of the global optimum.\nAdditionally, the deceptive solution should be\ncloser to the global optimum, so that it can\nshadow the global optimum for LKH. Thus,we filter out all the possible potential solutions\nbased on the following metric:\ndNBD(n)\u226510\u2227 \u2225n,o\u2225 \u226417 (14)\nNext, we analyze the local structure around\nthe potential deceptive solution and the global\noptimum. We performed local sampling around\nboth the deceptive solution and the global op-\ntimum, resulting in two solution sets, S(n, K)\nandS(o, K)with a sampling radius of K=\n17. Then, we analyze the difference of the\naverage fitness of the two solution sets, denoted\nas\u2206fin TABLE IV, calculated using the fol-\nlowing formula:\n\u2206f=P\nxi\u2208S(o,K)f(xi)\nN\u2212P\nxi\u2208S(n,K)f(xi)\nN(15)\nFrom TABLE IV, we observe that only two\nfunnels have better local areas than the global\noptimum: Funnel 3 of u574 and Funnel 4 of\nrue500-2. Interestingly, Funnel 4 of rue500-2\nis the deceptive solution that we predicted,\ni.e., the blue diamond-shaped solution in Fig. 8.\nThis indicates that both funnels are potentially\ndeceptive solutions.\nWe need to further analyze whether the BoAs\nof the two funnels are close to the BoA of\nthe optima so that the solutions in the BoA of\nthe global optimum are easily attracted by the\nfunnel. By analyzing the NBN of the combined\ndata of the two solution sets, as shown in\n\n--- Page 15 ---\n15\ntop view side view\n(a) u574, Id = 3\ntop view side view\n(b) rue500-2, Id = 4\nFig. 9. NBN of the combined data of N= 106solutions from local sampling with a radius of K= 17 around the possible deceptive\nsolution and global optima, where the blue rectangles represent the solutions around the possible deceptive solution and the green rectangles\nrepresent the solutions around the optima.\nTABLE IV, we found out that the sampled\nsolution set of funnel 4 of rue500-2 is closer\nto the global optimum compared to funnel 3 of\nu574, with a smaller d(S(n, K),o). This dis-\ntance is even shorter when compared to some\nother funnels, particularly in a larger sampled\nsolution set ( N= 1e6). Furthermore, Fig. 9\nalso shows that many solutions connect the two\nfunnels in NBN of rue500-2, Id = 4 than in\nu574, Id = 3. This suggests that the solutions\naround the global optimum in rue500-2 are\nmore easily attracted to funnel 4.\nNote that the NBN created using the two\nlocal sampling datasets is biased, with few\nsolutions located at the center of the funnel\nand the global optima. Therefore, the distance\nd(S(n, K),o)may not be accurate, and the\ntrue value is likely smaller. However, for dif-\nferent funnels, the distribution of the sampled\nsolutions remains consistent, making the dis-\ntances d(S(n, K),o)of different funnels com-\nparable.\n5) Conclusions from the Analysis: Based on the\nNBN-assisted analysis, three primary challenges are\nidentified for TSP: ruggedness, modality, and deception.\nMost TSP local search operators can overcome rugged-\nness challenges, as NBN structures generated from op-\ntimization data are remarkably smoother compared to\nrandomly sampled NBN structures.\nEAX struggles with the modality challenge. EAX\ncan efficiently maintain diversity, but when there are\nmany optima in the problem, solutions are distributed in\ndifferent BoAs. Few solutions are located in the BoAs of\nthe global optima and, therefore, it is hard to converge to\nthe global optima. While LKH does not suffer from the\nmodality challenge. It relies on its local search operators\nand can converge to different optima with a randomrestart in each run. LKH struggles with the deception\nchallenge. When there is a deceptive solution near the\nglobal optima, the local-search-based LKH tends to\nconverge to the deceptive solution. While EAX just treats\nit as a local optimum.\nVI. C ONCLUSIONS AND FUTURE WORK\nIn this paper, we offered a straightforward proof indi-\ncating that NBN fundamentally represents the maximum\nprobability transition network. We also presented an effi-\ncient calculation method for NBN with logarithmic linear\ntime complexity for assignment problems. Furthermore,\nwe conducted an in-depth analysis in OneMax problems\nand TSP. For the first time, we found that the fitness\nlandscape of OneMax exhibits neutrality, ruggedness,\nand modality features. We also uncovered some limita-\ntions of the state-of-the-art TSP algorithms: LKH, which\nrelies on its local search operators, fails when there are\ndeceptive solutions near the global optima. While, EAX,\nbased on a single population, efficiently maintains di-\nversity. However, when multiple attraction basins exist, it\nretains individuals within multiple basins simultaneously,\nreducing inter-basin interaction efficiency and leading to\nalgorithm\u2019s stagnation as well.\nWe believe that since NBN can reveal the underlying\nchallenges of the problems, it can also solve them.\nTo tackle the limitations of current TSP algorithms in\ndealing with modality and deception challenges, we aim\nto develop NBN-based algorithms capable of adaptively\nlearning these landscape features.\nREFERENCES\n[1] G. Laporte, \u201cThe vehicle routing problem: An overview of exact\nand approximate algorithms,\u201d European journal of operational\nresearch , vol. 59, no. 3, pp. 345\u2013358, 1992.\n[2] M. Pop, \u201cGenome assembly reborn: recent computational chal-\nlenges,\u201d Briefings in bioinformatics , vol. 10, no. 4, pp. 354\u2013366,\n2009.\n\n--- Page 16 ---\n16\n[3] C. J. Alpert, D. P. Mehta, and S. S. Sapatnekar, Eds., Handbook\nof algorithms for physical design automation . CRC press,\n2008.\n[4] R. Tin \u00b4os, K. Helsgaun, and D. Whitley, \u201cEfficient recombination\nin the lin-kernighan-helsgaun traveling salesman heuristic,\u201d in\nParallel Problem Solving from Nature\u2013PPSN XV: 15th Interna-\ntional Conference, Coimbra, Portugal, September 8\u201312, 2018,\nProceedings, Part I 15 . Springer, 2018, pp. 95\u2013107.\n[5] Y . Nagata and S. Kobayashi, \u201cA powerful genetic algorithm\nusing edge assembly crossover for the traveling salesman prob-\nlem,\u201d INFORMS Journal on Computing , vol. 25, no. 2, pp. 346\u2013\n363, 2013.\n[6] J. Scholz, \u201cGenetic algorithms and the traveling salesman\nproblem a historical review,\u201d arXiv preprint arXiv:1901.05737 ,\n2019.\n[7] S. Liu, Y . Zhang, K. Tang, and X. Yao, \u201cHow good is neural\ncombinatorial optimization? a systematic evaluation on the\ntraveling salesman problem,\u201d IEEE Computational Intelligence\nMagazine , vol. 18, no. 3, pp. 14\u201328, 2023.\n[8] K. Michalak, \u201cLow-dimensional euclidean embedding for visu-\nalization of search spaces in combinatorial optimization,\u201d IEEE\nTransactions on Evolutionary Computation , vol. 23, no. 2, pp.\n232\u2013246, 2019.\n[9] G. Ochoa and N. Veerapen, \u201cMapping the global structure of\ntsp fitness landscapes,\u201d Journal of Heuristics , vol. 24, no. 3, p.\n265\u2013294, jun 2018.\n[10] Y . Diao, C. Li, S. Zeng, and S. Yang, \u201cNearest better network\nfor visualization of the fitness landscape,\u201d in Proceedings of the\nCompanion Conference on Genetic and Evolutionary Compu-\ntation , ser. GECCO \u201923 Companion. New York, NY , USA:\nAssociation for Computing Machinery, 2023, p. 815\u2013818.\n[11] Y . Diao, C. Li, S. Zeng, S. Yang, and C. A. C. Coello, \u201cNearest-\nbetter network for fitness landscape analysis of continuous\noptimization problems,\u201d IEEE Transactions on Evolutionary\nComputation , 2024, early Access.\n[12] T. Weise and Z. Wu, \u201cDifficult features of combinatorial\noptimization problems and the tunable w-model benchmark\nproblem for simulating them,\u201d in Proceedings of the Genetic\nand Evolutionary Computation Conference Companion , ser.\nGECCO \u201918. New York, NY , USA: Association for Computing\nMachinery, 2018, p. 1769\u20131776.\n[13] P. F. Stadler, Fitness landscapes . Berlin, Heidelberg: Springer\nBerlin Heidelberg, 2002, pp. 183\u2013204.\n[14] F. Zou, D. Chen, H. Liu, S. Cao, X. Ji, and Y . Zhang, \u201cA survey\nof fitness landscape analysis for optimization,\u201d Neurocomputing ,\nvol. 503, pp. 129\u2013139, 2022.\n[15] M. Lipsitch, \u201cAdaptation on rugged landscapes generated by\niterated local interactions of neighboring genes,\u201d in Proceedings\nof the 4th International Conference on Genetic Algorithms, San\nDiego, CA, USA, July 1991 , R. K. Belew and L. B. Booker, Eds.\nMorgan Kaufmann, 1991, pp. 128\u2013135.\n[16] Y . Davidor, \u201cEpistasis variance: A viewpoint on ga-hardness,\u201d\ninFoundations of Genetic Algorithms , G. J. RAWLINS, Ed.\nElsevier, 1991, vol. 1, pp. 23\u201335.\n[17] C. M. Reidys and P. F. Stadler, \u201cNeutrality in fitness land-\nscapes,\u201d Applied Mathematics and Computation , vol. 117, no. 2,\npp. 321\u2013350, 2001.\n[18] M. Lunacek and L. D. Whitley, \u201cThe dispersion metric and\nthe CMA evolution strategy,\u201d in Genetic and Evolutionary\nComputation Conference, GECCO 2006, Proceedings, Seattle,\nWashington, USA, July 8-12, 2006 , M. Cattolico, Ed. ACM,\n2006, pp. 477\u2013484.\n[19] C. Li, T. T. Nguyen, S. Zeng, M. Yang, and M. Wu, \u201cAn open\nframework for constructing continuous optimization problems,\u201d\nIEEE Transactions on Cybernetics , vol. 49, no. 6, pp. 2316\u2013\n2330, 2019.[20] G. Ochoa, S. Verel, F. Daolio, and M. Tomassini, Local Optima\nNetworks: A New Model of Combinatorial Fitness Landscapes .\nBerlin, Heidelberg: Springer Berlin Heidelberg, 2014, pp. 233\u2013\n262.\n[21] C. S. Hsu, Cell-to-cell mapping: a method of global analysis\nfor nonlinear systems . Springer Science & Business Media,\n2013, vol. 64.\n[22] N. Hansen and A. Ostermeier, \u201cCompletely derandomized self-\nadaptation in evolution strategies,\u201d Evolutionary Computation ,\nvol. 9, no. 2, pp. 159\u2013195, 06 2001.\n[23] M. Preuss, \u201cNiching the CMA-ES via nearest-better clustering,\u201d\ninGenetic and Evolutionary Computation Conference, GECCO\n2010, Proceedings, Portland, Oregon, USA, July 7-11, 2010,\nCompanion Material , M. Pelikan and J. Branke, Eds. ACM,\n2010, pp. 1711\u20131718.\n[24] S. Raggl, A. Beham, V . A. Hauder, S. Wagner, and M. Affen-\nzeller, \u201cDiscrete real-world problems in a black-box optimiza-\ntion benchmark,\u201d in Proceedings of the Genetic and Evolution-\nary Computation Conference Companion, GECCO 2018, Kyoto,\nJapan, July 15-19, 2018 , H. E. Aguirre and K. Takadama, Eds.\nACM, 2018, pp. 1745\u20131752.\n[25] L. R. Dice, \u201cMeasures of the amount of ecologic association\nbetween species,\u201d Ecology , vol. 26, no. 3, pp. 297\u2013302, 1945.\n[26] M. Dorigo and T. St \u00a8utzle, Ant Colony Optimization: Overview\nand Recent Advances . Cham: Springer International Publish-\ning, 2019, pp. 311\u2013351.\n[27] E. Bingham and H. Mannila, \u201cRandom projection in dimen-\nsionality reduction: Applications to image and text data,\u201d\ninProceedings of the Seventh ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , ser.\nKDD \u201901. New York, NY , USA: Association for Computing\nMachinery, 2001, p. 245\u2013250.\n[28] J. M. Kleinberg, \u201cTwo algorithms for nearest-neighbor search in\nhigh dimensions,\u201d in Proceedings of the Twenty-Ninth Annual\nACM Symposium on Theory of Computing , ser. STOC \u201997.\nNew York, NY , USA: Association for Computing Machinery,\n1997, p. 599\u2013608.\n[29] G. Reinelt, \u201cTsplib\u2014a traveling salesman problem library,\u201d\nORSA Journal on Computing , vol. 3, no. 4, pp. 376\u2013384, 1991.\n[30] G. Gutin and A. Punnen, The Traveling Salesman Problem and\nIts Variations , ser. Combinatorial Optimization. Springer US,\n2006.",
  "project_dir": "artifacts/projects/enhanced_cs.NE_2507.22440v1_Nearest_Better_Network_for_Visualizing_and_Analyzi",
  "communication_dir": "artifacts/projects/enhanced_cs.NE_2507.22440v1_Nearest_Better_Network_for_Visualizing_and_Analyzi/.agent_comm",
  "assigned_at": "2025-07-31T21:53:40.392319",
  "status": "assigned"
}